{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02180fd1-9814-44ff-96f9-8d444a7ea880",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb2263-f678-4911-b59c-57095705de20",
   "metadata": {},
   "source": [
    "Now that we have a fundamental understanding of derivatives, we can apply it to a widely used algorithm across optimization and machine learning. **Gradient descent** allows us to minimize or maximize a function using an iterative process leveraging the slope of each variable. Typically for machine learning, we are trying to minimize a loss function or maximize likelihood. We will start with a trivial example and then apply it to linear regression. We will also talk about stochastic gradient descent and what to expect with more complex models like deep learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ea07a9-7a09-44ac-b010-dc78d2b03dff",
   "metadata": {},
   "source": [
    "## Understanding Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6d2f1d-e89c-4acc-b711-558740fc8dde",
   "metadata": {},
   "source": [
    "Imagine you were in a mountain range at night with only a flashlight. You know that to get back to town safely, you have to get to the valley, the lowest point in the mountain range. You use the flashlight to eye the slope around you in all directions, and you step in the direction that descends the most. You take bigger steps for bigger slopes, and smaller steps for smaller slopes. Eventually, you will get to a low point in the mountain range **where the slope is 0**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39d0d4-fd54-413e-91a1-074d10d6b101",
   "metadata": {},
   "source": [
    "![img](data:image/svg+xml;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c308ab-ae91-49b1-821f-1d03a3d4db7a",
   "metadata": {},
   "source": [
    "Now consider a case where there is only one valley, or one minimum, in the entire mountain range. We call this a **convex** problem because there is only one minimum. Problems that are convex include linear regression, logistic regression, and linear programming problems. These are fairly straightforward to solve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bfa002-31cf-4e54-8aae-1dff98d01e7e",
   "metadata": {},
   "source": [
    "![img](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   width="94.032326mm"
   height="72.158356mm"
   viewBox="0 0 94.032323 72.158355"
   version="1.1"
   id="svg1"
   inkscape:version="1.3 (0e150ed, 2023-07-21)"
   sodipodi:docname="gradient_night.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview1"
     pagecolor="#ffffff"
     bordercolor="#000000"
     borderopacity="0.25"
     inkscape:showpageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:deskcolor="#d1d1d1"
     inkscape:document-units="mm"
     showguides="false"
     inkscape:zoom="1.5519601"
     inkscape:cx="194.59264"
     inkscape:cy="92.785893"
     inkscape:window-width="1472"
     inkscape:window-height="891"
     inkscape:window-x="0"
     inkscape:window-y="37"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs1" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-42.208559,-7.3650704)">
    <rect
       style="fill:#000000;stroke:#000000;stroke-width:0.0867177;stroke-linecap:round;stroke-linejoin:round"
       id="rect3"
       width="93.963081"
       height="72.062286"
       x="42.277805"
       y="7.4083939" />
    <path
       id="path4"
       style="fill:#ffff00;stroke:#000000;stroke-width:0.1;stroke-linecap:round;stroke-linejoin:round"
       d="m 117.79602,15.627192 a 6.1735773,6.1735773 0 0 0 -6.17379,6.173784 6.1735773,6.1735773 0 0 0 6.17379,6.173266 6.1735773,6.1735773 0 0 0 2.85305,-0.715202 5.443975,6.1985774 0 0 1 -2.85305,-5.43326 5.443975,6.1985774 0 0 1 2.91041,-5.469433 6.1735773,6.1735773 0 0 0 -2.91041,-0.729155 z" />
    <g
       id="g2"
       transform="matrix(0.43100203,0,0,0.43301315,24.893057,12.796755)">
      <path
         style="fill:#999999;stroke:#808080;stroke-width:0.100001;stroke-linecap:round;stroke-linejoin:round;opacity:1"
         d="m 40.245732,79.469697 -0.07074,-36.479151 c 0,0 0.06745,-45.1348978 0.110772,-47.6977438 0.101036,-5.9767542 13.150995,9.516355 13.150995,9.516355 0,0 10.999227,10.9067728 13.349053,7.9586058 4.482583,-5.6239912 11.563114,8.357677 13.359094,12.948283 1.79598,4.590607 11.767148,26.133435 18.184302,25.91751 6.671862,-0.224495 22.674242,7.632786 22.674242,10.775697 0,3.142911 13.24247,19.725527 22.67136,19.501033 9.4289,-0.224494 -103.429075,-2.440589 -103.429075,-2.440589 z"
         id="path3"
         sodipodi:nodetypes="ccscssssscc" />
      <g
         id="g1">
        <ellipse
           style="fill:#ffffff;stroke:#ffffff;stroke-width:0.1;stroke-linecap:round;stroke-linejoin:round"
           id="path5"
           cx="106.29778"
           cy="36.592476"
           rx="2.8061717"
           ry="2.9184184" />
        <path
           style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 106.29778,39.510893 v 9.877724"
           id="path6" />
        <path
           style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 106.29778,39.510893 5.67432,4.93886"
           id="path7" />
        <path
           style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 106.29778,49.388617 2.83716,6.510319"
           id="path8" />
        <path
           style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 106.29778,49.388617 -2.85617,6.860317"
           id="path9" />
        <path
           style="fill:#666666;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 106.29778,39.510893 -3.20617,6.959306"
           id="path10" />
        <path
           style="fill:#0000ff;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 101.69615,45.467938 2.99854,1.83493 -1.05994,1.735749 -2.61393,-2.667643 z"
           id="path11"
           sodipodi:nodetypes="ccccc" />
        <path
           style="opacity:0.633423;fill:#ffff00;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
           d="m 103.63475,49.038615 8.68735,11.164642 8.68001,2.205995 -16.30742,-15.106385 z"
           id="path12" />
      </g>
    </g>
    <path
       style="opacity:1;fill:#999999;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       d="m 83.32914,47.208216 6.976107,2.11354 c 0,0 4.531908,1.023333 5.262859,3.947143 0.730953,2.923813 5.116664,7.894291 7.748094,10.233338 2.63144,2.339049 6.44903,4.385717 9.64858,4.093337 3.19955,-0.292381 13.30334,-3.800954 13.74191,-6.870957 0.43857,-3.07 2.48524,-10.215327 3.21619,-17.285081 0.73096,-7.069754 0.11439,-18.531601 3.2162,-24.086841 3.1018,-5.555241 3.1018,-9.210004 3.1018,-9.210004 l 1e-5,69.327988 -94.032331,0.05275 0.03049,-32.31525 z"
       id="path2"
       sodipodi:nodetypes="ccssssssccccc" />
    <g
       id="g16">
      <rect
         style="opacity:1;fill:#800000;stroke:#800000;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
    <g
       id="g16-3"
       transform="translate(6.6523361,-1.7371174)">
      <rect
         style="opacity:1;fill:#784421;stroke:#784421;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2-9"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13-8" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13-6"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14-5"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15-7"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16-6"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
    <g
       id="g16-9"
       transform="translate(-7.2778015,-0.34661447)">
      <rect
         style="opacity:1;fill:#800000;stroke:#800000;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2-99"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13-1" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13-7"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14-2"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15-3"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16-65"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
  </g>
</svg>
)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14eb2c7-3ecd-4749-93a5-e16d87a63179",
   "metadata": {},
   "source": [
    "Now consider a landscape that has several **local minima**, or multiple valleys we can get stuck in. Because these valleys will not reveal a slope that descends further, it is easy for gradient descent to get stuck in them. We call these types of problems **non-convex problems**, and they are much harder to solve. Problems that are non-convex include neural networks and deep learning. Typically stochastic gradient descent and other random-based techniques are used to cope with non-convex, and we will talk about this later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cdaeee-903a-4329-836f-2f274cadd2e8",
   "metadata": {},
   "source": [
    "![img](data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!-- Created with Inkscape (http://www.inkscape.org/) -->

<svg
   width="94.032326mm"
   height="72.158356mm"
   viewBox="0 0 94.032323 72.158355"
   version="1.1"
   id="svg1"
   inkscape:version="1.3 (0e150ed, 2023-07-21)"
   sodipodi:docname="gradient_night.svg"
   xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"
   xmlns:sodipodi="http://sodipodi.sourceforge.net/DTD/sodipodi-0.dtd"
   xmlns="http://www.w3.org/2000/svg"
   xmlns:svg="http://www.w3.org/2000/svg">
  <sodipodi:namedview
     id="namedview1"
     pagecolor="#ffffff"
     bordercolor="#000000"
     borderopacity="0.25"
     inkscape:showpageshadow="2"
     inkscape:pageopacity="0.0"
     inkscape:pagecheckerboard="0"
     inkscape:deskcolor="#d1d1d1"
     inkscape:document-units="mm"
     showguides="false"
     inkscape:zoom="1.8819002"
     inkscape:cx="169.50952"
     inkscape:cy="133.37583"
     inkscape:window-width="1472"
     inkscape:window-height="891"
     inkscape:window-x="0"
     inkscape:window-y="37"
     inkscape:window-maximized="1"
     inkscape:current-layer="layer1" />
  <defs
     id="defs1" />
  <g
     inkscape:label="Layer 1"
     inkscape:groupmode="layer"
     id="layer1"
     transform="translate(-42.208559,-7.3650704)">
    <rect
       style="fill:#000000;stroke:#000000;stroke-width:0.0867177;stroke-linecap:round;stroke-linejoin:round"
       id="rect3"
       width="93.963081"
       height="72.062286"
       x="42.277805"
       y="7.4083939" />
    <path
       id="path4"
       style="fill:#ffff00;stroke:#000000;stroke-width:0.1;stroke-linecap:round;stroke-linejoin:round"
       d="m 117.79602,15.627192 a 6.1735773,6.1735773 0 0 0 -6.17379,6.173784 6.1735773,6.1735773 0 0 0 6.17379,6.173266 6.1735773,6.1735773 0 0 0 2.85305,-0.715202 5.443975,6.1985774 0 0 1 -2.85305,-5.43326 5.443975,6.1985774 0 0 1 2.91041,-5.469433 6.1735773,6.1735773 0 0 0 -2.91041,-0.729155 z" />
    <path
       style="opacity:1;fill:#999999;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
       d="m 42.208559,23.731471 c 0,0 8.524167,-0.300519 10.910934,3.976665 2.386766,4.277185 3.198929,11.022071 10.399483,10.066846 7.200553,-0.955225 10.941656,0 10.941656,0 17.737548,18.433196 21.245939,13.346498 26.235888,-6.541264 0.2645,-1.348792 4.24105,-4.63774 5.78426,-2.55823 2.39776,3.231046 0.82071,18.596231 8.03034,23.867668 7.20964,5.271433 16.86022,1.704832 16.86022,1.704832 0,0 2.55725,-4.008639 1.70484,-16.473006 -0.85242,-12.464368 3.1647,-20.010428 3.1647,-20.010428 l 1e-5,61.706125 -94.032331,0.05275 z"
       id="path16"
       sodipodi:nodetypes="csscssscscccc" />
    <g
       id="g1"
       transform="matrix(0.43100203,0,0,0.43301315,22.308478,10.750955)">
      <ellipse
         style="fill:#ffffff;stroke:#ffffff;stroke-width:0.1;stroke-linecap:round;stroke-linejoin:round"
         id="path5"
         cx="106.29778"
         cy="36.592476"
         rx="2.8061717"
         ry="2.9184184" />
      <path
         style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 106.29778,39.510893 v 9.877724"
         id="path6" />
      <path
         style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 106.29778,39.510893 5.67432,4.93886"
         id="path7" />
      <path
         style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 106.29778,49.388617 2.83716,6.510319"
         id="path8" />
      <path
         style="fill:#ffffff;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 106.29778,49.388617 -2.85617,6.860317"
         id="path9" />
      <path
         style="fill:#666666;stroke:#ffffff;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 106.29778,39.510893 -3.20617,6.959306"
         id="path10" />
      <path
         style="fill:#0000ff;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 101.69615,45.467938 2.99854,1.83493 -1.05994,1.735749 -2.61393,-2.667643 z"
         id="path11"
         sodipodi:nodetypes="ccccc" />
      <path
         style="opacity:0.633423;fill:#ffff00;stroke:none;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round;stroke-dasharray:none"
         d="m 103.63475,49.038615 8.68735,12.420578 8.68001,0.950059 -16.30742,-15.106385 z"
         id="path12"
         sodipodi:nodetypes="ccccc" />
    </g>
    <g
       id="g16"
       transform="translate(9.5738528,-12.2748)">
      <rect
         style="opacity:1;fill:#800000;stroke:#800000;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
    <g
       id="g16-3"
       transform="translate(16.226189,-14.011918)">
      <rect
         style="opacity:1;fill:#784421;stroke:#784421;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2-9"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13-8" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13-6"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14-5"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15-7"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16-6"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
    <g
       id="g16-9"
       transform="translate(2.2960513,-12.621415)">
      <rect
         style="opacity:1;fill:#800000;stroke:#800000;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect2-99"
         width="4.5920973"
         height="5.5552406"
         x="111.57223"
         y="62.332714" />
      <path
         style="opacity:1;fill:#0000ff;stroke:#0000ff;stroke-width:0.491473;stroke-linecap:round;stroke-linejoin:round"
         d="m 111.46797,62.436981 v 0 l 2.40031,-2.343775 2.4003,2.343775 z"
         id="path13-1" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect13-7"
         width="0.57178134"
         height="0.88085234"
         x="115.21113"
         y="63.353283" />
      <rect
         style="opacity:1;fill:#0000ff;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect14-2"
         width="0.57178134"
         height="0.71086329"
         x="112.19769"
         y="64.172325" />
      <rect
         style="opacity:1;fill:#483737;stroke:#ffff00;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect15-3"
         width="0.57178134"
         height="0.88085234"
         x="113.75849"
         y="65.887665" />
      <rect
         style="opacity:1;fill:#483737;stroke:#483737;stroke-width:0.7;stroke-linecap:round;stroke-linejoin:round"
         id="rect16-65"
         width="0.20730175"
         height="1.7133832"
         x="114.86112"
         y="59.551708" />
    </g>
  </g>
</svg>
)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d22d58-c54a-4309-a05a-b3884393a926",
   "metadata": {},
   "source": [
    "Metaphorically, the landscape is a mathematical function we are trying to find the lowest point for.\n",
    "\n",
    "Let's apply gradient descent to a simple problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8867e4-2a98-493e-a5fd-46c3c3eba7ec",
   "metadata": {},
   "source": [
    "## Simple Gradient Descent Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70441c31-bd98-4812-b1c5-ceef4e293ae6",
   "metadata": {},
   "source": [
    "Let's take this function: \n",
    "\n",
    "$\n",
    "f(x) = 3 \\left(x + 1\\right)^{2} + 1\n",
    "$ \n",
    "\n",
    "Here it is plotted in SymPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd0820f-687d-438b-b62c-c6abd33f8d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "\n",
    "x = symbols('x')\n",
    "f = 3*(x+1)**2 + 1\n",
    "plot(f, xlim=(-3,1), ylim=(-1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8024ed35-2fd5-4a2f-a4a6-7047b6512244",
   "metadata": {},
   "source": [
    "We could solve this algebraically by first taking the derivative of the function with respect to $ x $, and then solve for where the slope is $ 0 $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38f5057-eac9-41ff-ab40-d98ad3d9368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.solvers import solve \n",
    "\n",
    "x = symbols('x')\n",
    "f = 3*(x+1)**2 + 1\n",
    "dx = diff(f, x)\n",
    "\n",
    "solve(dx, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68946fc-069b-44e7-b4ed-9fab206b1e55",
   "metadata": {},
   "source": [
    "While we have a shortcut to solve this simple problem, we do not have shortcuts for more complex machine learning problems where we need to use gradient descent. But we can understand gradient descent first by applying it to a simple problem like this one. \n",
    "\n",
    "We are still going to use the derivative of the function, but we are going to start a searching algorithm at a random location for $ x $ that is reasonably in the vicinity of the solution. Below we will initialize a random $ x $ but contain it in the range of our graph above for visual consistency. We will plot the tangent line for that starting $ x $ location. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c872e2-7e53-4519-8639-ccd41d52255c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "# start x at random location \n",
    "x_i = random.uniform(-3,1)\n",
    "\n",
    "# calculate slope at random x \n",
    "# and tangent line \n",
    "m = dx.subs(x, x_i) \n",
    "b = -(m * x_i - f.subs(x, x_i))\n",
    "\n",
    "plot(f, m*x+b, xlim=(-3,1), ylim=(-1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbfbbad-1ab7-422e-a177-24dbcdfef828",
   "metadata": {},
   "source": [
    "Let's declare a **learning rate** of $ .05 $, which takes a fraction of the slope and subtracts it from our x-value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deddb2e9-5f5e-44c7-9147-0473009d50e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = .05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339ba45-d4f7-4ab6-942d-7fecfb691c6b",
   "metadata": {},
   "source": [
    "**Now run this code block below several time and watch what happens**. Pay attention to the $ x $ value and the tangent line. Where is it converging? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454317c6-1d56-4e2c-a930-facf522960b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code cell repeatedly \n",
    "x_i -= m * L \n",
    "m = dx.subs(x, x_i) \n",
    "b = -(m * x_i - f.subs(x, x_i))\n",
    "print(f\"x = {x_i}\")\n",
    "plot(f, m*x+b, xlim=(-3,1), ylim=(-1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a72e2-805f-4e05-bb18-b842fc07061a",
   "metadata": {},
   "source": [
    "This searching for an $ x $ that gives us a slope of $ 0 $ on the tangent line, and thus the miniumum, is what we call gradient descent. Let's repackage all the code above into a `for` loop that does this 1000 times. You will see the line converges on the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197734c-cf15-4592-bca1-f9923e68b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.solvers import solve \n",
    "import random \n",
    "\n",
    "# declare function and derivative\n",
    "x = symbols('x')\n",
    "f = 3*(x+1)**2 + 1\n",
    "dx = diff(f, x) \n",
    "\n",
    "# declare learning rate \n",
    "L = .05\n",
    "\n",
    "# start x at random location \n",
    "x_i = random.uniform(-3,1)\n",
    "\n",
    "for i in range(1000):a\n",
    "    x_i -= m * L \n",
    "    m = dx.subs(x, x_i) \n",
    "    b = -(m * x_i - f.subs(x, x_i))\n",
    "\n",
    "print(f\"x = {x_i}\")\n",
    "plot(f, m*x+b, xlim=(-3,1), ylim=(-1,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2af46e-a081-439e-afef-52f4307dbf84",
   "metadata": {},
   "source": [
    "So in the end, gradient descent is starting at a random location in our function (a random $ x $) and repeatedly subtracting the slope times the learning rate. \n",
    "\n",
    "But how do we choose a learning rate? \n",
    "\n",
    "### Choosing a Learning Rate \n",
    "\n",
    "The **learning rate** sets how aggressively you want the gradient descent algorithm to move towards the minimum. It is a fraction of the slope subtracted from the $ x $ value repeatedly until the function is minimized (if you are maximizing function, add the slope times the learning rate rather than subtract).\n",
    "\n",
    "The larger the learning rate, the faster progress will happen but at the cost of accuracy. If it is too large, it may not converge to the minuimum at all as it would be like a giant stepping over the valley over and over. Having it too small will create more precision, but require more time and steps much like an ant descending into the valley. You need to find a balance of the two. Experiment with the learning rate above (e.g. $ .3 $ versus $ .001 $) to see how it affects the progress of gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a49cf8-ab69-41d1-9737-f47c2a412bff",
   "metadata": {},
   "source": [
    "## Multivariable Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5773611c-ff4c-4994-89db-f4ef7cb09a7a",
   "metadata": {},
   "source": [
    "Let's look at this multivariable function and plot it. \n",
    "\n",
    "$\n",
    "f(x) = 5 x^{2} + 4 y^{2} + 1\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37132f21-48f4-42df-bcaf-411b533e4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "\n",
    "x, y = symbols('x y')\n",
    "\n",
    "f = 5*x**2 + 4*y**2 + 1 \n",
    "\n",
    "plot3d(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a642aa64-b393-4775-8535-d6f0d6bb81d2",
   "metadata": {},
   "source": [
    "Again, we can algebraically solve for the minimum but let's practice using gradient descent with it. As we learned in the last section we can use partial derivatives to find the derivative with respect to each input variable. \n",
    "\n",
    "$ \n",
    "\\Large \\frac{\\delta}{\\delta x} = 10x\n",
    "$   \n",
    "\n",
    "$ \n",
    "\\Large \\frac{\\delta}{\\delta y} = 8y\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8097a61-a68b-45d1-bf5d-cfd11024de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "\n",
    "x, y = symbols('x y')\n",
    "\n",
    "f = 5*x**2 + 4*y**2 + 1 \n",
    "dx = diff(f, x) \n",
    "dy = diff(f, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bda0ad-3c7a-4f87-8cb0-c4312d8f4a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b86aa80-e340-4eb3-a816-41fda755edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e29a58-6fd8-4378-aa64-b9c8415379b3",
   "metadata": {},
   "source": [
    "Let's experiment with gradient descent in a similar manner we did earlier. First we declare our partial derivatives and a learning rate $ L = .05 $. We will also start $ x $ and $ y $ in a random location in the chart above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38277c9f-cc7e-458c-962a-5f244cfe9fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "import random \n",
    "\n",
    "# declare function and derivative\n",
    "x,y = symbols('x y')\n",
    "f = 5*x**2 + 4*y**2 + 1 \n",
    "dx = diff(f, x) \n",
    "dy = diff(f, y)\n",
    "\n",
    "# declare learning rate \n",
    "L = .05\n",
    "\n",
    "# start x at random location \n",
    "x_i = random.uniform(-10,10)\n",
    "y_i = random.uniform(-10,10)\n",
    "\n",
    "# plot \n",
    "dx_i = dx.subs(x, x_i)\n",
    "dy_i = dy.subs(y, y_i) \n",
    "\n",
    "plot3d(f, dx_i*x + dy_i * y + b, xlim=(-10,10),ylim=(-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce45891e-e462-4b35-93b7-5aaf33ac51b6",
   "metadata": {},
   "source": [
    "Run this block of code below repeatedly and you will see the linear plane, capturing the slope for both $ x $ and $ y $, drifting towards the minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b4b37-bd73-43e4-bbc3-b676feadb939",
   "metadata": {},
   "outputs": [],
   "source": [
    "dx_i = dx.subs(x, x_i)\n",
    "dy_i = dy.subs(y, y_i) \n",
    "\n",
    "x_i -= dx_i * L \n",
    "y_i -= dy_i * L \n",
    "\n",
    "b = -(dx_i * x_i + dy_i * y_i - f.subs([(x, x_i), (y, y_i)]))\n",
    "\n",
    "print(f\"x = {x_i}, y = {y_i}\")\n",
    "plot3d(f, dx_i*x + dy_i * y + b, xlim=(-10,10),ylim=(-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d526f7d1-aa54-4473-93bc-52c809ed4bbe",
   "metadata": {},
   "source": [
    "We can repackage all of this into a single Python script to perform this gradient descent. You will see that $ x $ and $ y $ converge very closely to $ (0,0) $. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb2e906-ac3e-4bf7-9a7b-2886167f2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "import random \n",
    "\n",
    "# declare function and derivative\n",
    "x,y = symbols('x y')\n",
    "f = 5*x**2 + 4*y**2 + 1 \n",
    "dx = diff(f, x) \n",
    "dy = diff(f, y)\n",
    "\n",
    "# declare learning rate \n",
    "L = .05\n",
    "\n",
    "# start x at random location \n",
    "x_i = random.uniform(-10,10)\n",
    "y_i = random.uniform(-10,10)\n",
    "\n",
    "for i in range(1000):\n",
    "    dx_i = dx.subs(x, x_i)\n",
    "    dy_i = dy.subs(y, y_i) \n",
    "\n",
    "    x_i -= dx_i * L \n",
    "    y_i -= dy_i * L \n",
    "\n",
    "    b = -(dx_i * x_i + dy_i * y_i - f.subs([(x, x_i), (y, y_i)]))\n",
    "\n",
    "print(f\"x = {x_i}, y = {y_i}\")\n",
    "plot3d(f, dx_i*x + dy_i * y + b, xlim=(-10,10),ylim=(-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee87c41-7943-4596-90e4-d1b5a5d4f17e",
   "metadata": {},
   "source": [
    "## Gradient Descent for Linear Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b1663-ea97-47c9-a213-576fa507df5e",
   "metadata": {},
   "source": [
    "Let's apply gradient descent to something a bit closer to real-world practice. While linear regression does have shortcut techniques like matrix decomposition, it is a good way to understand gradient descent for data-driven machine learning models. After all, neural networks are composed of linear functions inside nonlinear functions, whose slopes and intercepts (or weights and biases) are optimized with gradient descent. Let's practice with a single linear function.\n",
    "\n",
    "A linear regression fits a line (or linear plane if there are multiple input variables) through some data. The **loss function** is what we are trying to minimize using gradient descent, and it typically will be the *sum of squares* or *mean of squares*. The **squared residuals** are the squared differences between each data point's $ y $ value and predicted $ y $ value from the line, which when summed or averaged make up the loss function. \n",
    "\n",
    "Here are the squared residuals visualized below for a given line and 15 data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41e5c2-729f-474d-8cf6-ef449ef64dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Extract input variables (all rows, all columns but last column)\n",
    "X = np.array([9.8, 8.3, 5.3, 1.3, 3, 0.4, 5.4, 7.3, 3.7, 6.8, 5.6, 2,7.6, 7.9, 1.5])\n",
    "\n",
    "Y = np.array([8.383017, 7.35061323, 5.31904498, 0.99811892, 2.64478489, 1.12535641,\n",
    " 5.62574367, 6.82704871, 5.66768037, 6.98267837, 7.23655439, 3.36467504,\n",
    " 9.82253924, 8.52430761, 1.39760223])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# plot squares \n",
    "for x,y in zip(X,Y): \n",
    "    residual = m*x+b - y\n",
    "    ax.add_patch(Rectangle((x, y), residual, residual, alpha=.5, color='orange'))\n",
    "\n",
    "# declare line coefficients\n",
    "m, b = 0.9153874397162779, 0.7861238923689651\n",
    "\n",
    "plt.plot(X, m*X+b)\n",
    "plt.plot(X, Y, 'o') # scatterplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461a12e1-52cf-4c1b-b739-a82a0513c606",
   "metadata": {},
   "source": [
    "Let's use the sum of squares (the total of all the square areas above) as our loss function. \n",
    "\n",
    "$\n",
    "\\Large \\text{SSE} = \\sum_{i=0}^{n} (m x_i + b - y_i)^{2}\n",
    "$ \n",
    "\n",
    "To see what the loss landscape looks like, let's use SymPy. As you can guess, we need to find the $ m $ and $ b $ values that will get us to the lowest point of this plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab56da9-df28-481c-a7a2-4308c11ad92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "from sympy.plotting import plot3d\n",
    "import pandas as pd\n",
    "\n",
    "m, b, i, n = symbols('m b i n')\n",
    "x, y = symbols('x y', cls=Function)\n",
    "\n",
    "sum_of_squares = Sum((m*x(i) + b - y(i)) ** 2, (i, 0, n)) \\\n",
    "    .subs(n, len(X) - 1).doit() \\\n",
    "    .replace(x, lambda i: X[i]) \\\n",
    "    .replace(y, lambda i: Y[i])\n",
    "\n",
    "plot3d(sum_of_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd898cb-71a3-46a1-b848-cf703ff86681",
   "metadata": {},
   "source": [
    "We will find the loss function's derivative with respect to $ m $ and with respect to $ b $ using SymPy. Notice how we can support multiple $ x $ and $ y $ values by specifying `cls=Function` to `symbols()`. We will then use the `Sum` operator to perform a summation that totals the squared differences between the actual $ y $ values and the predicted $ y $ values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d352b7bb-7dfa-4f40-bf50-a067f5fcb7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import *\n",
    "\n",
    "m, b, i, n = symbols('m b i n')\n",
    "x, y = symbols('x y', cls=Function)\n",
    "\n",
    "sum_of_squares = Sum((m*x(i) + b - y(i)) ** 2, (i, 0, n))\n",
    "\n",
    "d_m = diff(sum_of_squares, m)\n",
    "d_b = diff(sum_of_squares, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c716aa-d94c-4a5f-a7d6-35fe86ba10bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d762d5-fc19-4e08-b62a-e07417b96d0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc77262-5859-4bcd-addc-b857ecee1b5b",
   "metadata": {},
   "source": [
    "We can implement those two derivatives manually in NumPy as shown below, and use it to perform gradient descent. Notice how closely this resembles our multivariable gradient descent example earlier. We are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a19d2c3-cb9e-46d6-8f54-e06a3f24f02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sympy import *\n",
    "\n",
    "X = np.array([9.8, 8.3, 5.3, 1.3, 3, 0.4, 5.4, 7.3, 3.7, 6.8, 5.6, 2,7.6, 7.9, 1.5])\n",
    "\n",
    "Y = np.array([8.383017, 7.35061323, 5.31904498, 0.99811892, 2.64478489, 1.12535641,\n",
    " 5.62574367, 6.82704871, 5.66768037, 6.98267837, 7.23655439, 3.36467504,\n",
    " 9.82253924, 8.52430761, 1.39760223])\n",
    "\n",
    "# Building the model\n",
    "m = 1.0\n",
    "b = 1.0\n",
    "\n",
    "# The learning Rate\n",
    "L = .001\n",
    "\n",
    "# The number of iterations\n",
    "iterations = 100_000\n",
    "\n",
    "n = float(len(X))  # Number of elements in X\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(iterations):\n",
    "\n",
    "    # slope with respect to m\n",
    "    D_m = (2 * X * ((m * X + b) - Y)).sum()\n",
    "\n",
    "    # slope with respect to b\n",
    "    D_b = (2 * ((m * X + b) - Y)).sum()\n",
    "\n",
    "    # update m and b\n",
    "    m -= L * D_m\n",
    "    b -= L * D_b\n",
    "print(m, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d7be56-ba86-42fd-985e-447dc0ad1672",
   "metadata": {},
   "source": [
    "If you want to keep using SymPy, just subsitute the data points into the derivative functions. By the nature of the summation implementation in SymPy, you will need to call `doit()` and then `lambdify()` to efficiently compile the derivative functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae64037-7be5-43ac-b6c5-1f9a541f0ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sympy import *\n",
    "\n",
    "X = np.array([9.8, 8.3, 5.3, 1.3, 3, 0.4, 5.4, 7.3, 3.7, 6.8, 5.6, 2,7.6, 7.9, 1.5])\n",
    "\n",
    "Y = np.array([8.383017, 7.35061323, 5.31904498, 0.99811892, 2.64478489, 1.12535641,\n",
    " 5.62574367, 6.82704871, 5.66768037, 6.98267837, 7.23655439, 3.36467504,\n",
    " 9.82253924, 8.52430761, 1.39760223])\n",
    "\n",
    "\n",
    "m, b, i, n = symbols('m b i n')\n",
    "x, y = symbols('x y', cls=Function)\n",
    "\n",
    "sum_of_squares = Sum((m*x(i) + b - y(i)) ** 2, (i, 0, n))\n",
    "\n",
    "d_m = diff(sum_of_squares, m) \\\n",
    "    .subs(n, len(X) - 1).doit() \\\n",
    "    .replace(x, lambda i: X[i]) \\\n",
    "    .replace(y, lambda i: Y[i])\n",
    "\n",
    "d_b = diff(sum_of_squares, b) \\\n",
    "    .subs(n, len(X) - 1).doit() \\\n",
    "    .replace(x, lambda i: X[i]) \\\n",
    "    .replace(y, lambda i: Y[i])\n",
    "\n",
    "# compile using lambdify for faster computation\n",
    "d_m = lambdify([m, b], d_m)\n",
    "d_b = lambdify([m, b], d_b)\n",
    "\n",
    "# Building the model\n",
    "m = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# The learning Rate\n",
    "L = .001\n",
    "\n",
    "# The number of iterations\n",
    "iterations = 100_000\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(iterations):\n",
    "\n",
    "    # update m and b\n",
    "    m -= d_m(m,b) * L\n",
    "    b -= d_b(m,b) * L\n",
    "\n",
    "print(\"y = {0}x + {1}\".format(m, b))\n",
    "\n",
    "print(m, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a547bd43-4c8b-41da-bf60-437a9738b9e2",
   "metadata": {},
   "source": [
    "Now let's take a look at the result and plot it. Looks pretty good! That line seems to fit to the points pretty nicely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615004b7-34e8-49a8-bd4f-bab1e4a1f16d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "# plot squares \n",
    "for x,y in zip(X,Y): \n",
    "    residual = m*x+b - y\n",
    "    ax.add_patch(Rectangle((x, y), residual, residual, alpha=.5, color='orange'))\n",
    "\n",
    "plt.plot(X, m*X+b)\n",
    "plt.plot(X, Y, 'o') # scatterplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6583b1be-7303-4f22-9461-70f46097c89e",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9f0368-1d65-4d8d-9f33-97abdcb0ebe8",
   "metadata": {},
   "source": [
    "It would be remiss to not at least mention **stochastic gradient descent**, a variant of gradient descent that randomly samples only one or more training datapoints in each iteration. This is done because traversing the entire dataset can be computationally expensive for larger datasets and complex models like deep learning. Below, we only randomly sample one datapoint in each iteration. You will notice the line does not fit as aggressively, and some randomness will produce different $ m $ and $ b $ values each time. That is likely okay as another purpose is to prevent overfitting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e585128-b96a-4c06-8620-373d33c7050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random \n",
    "from sympy import *\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([9.8, 8.3, 5.3, 1.3, 3, 0.4, 5.4, 7.3, 3.7, 6.8, 5.6, 2,7.6, 7.9, 1.5])\n",
    "\n",
    "Y = np.array([8.383017, 7.35061323, 5.31904498, 0.99811892, 2.64478489, 1.12535641,\n",
    " 5.62574367, 6.82704871, 5.66768037, 6.98267837, 7.23655439, 3.36467504,\n",
    " 9.82253924, 8.52430761, 1.39760223])\n",
    "\n",
    "# Building the model\n",
    "m = 1.0\n",
    "b = 1.0\n",
    "\n",
    "# The learning Rate\n",
    "L = .001\n",
    "\n",
    "# The number of iterations\n",
    "iterations = 100_000\n",
    "\n",
    "n = float(len(X))  # Number of elements in X\n",
    "\n",
    "# Perform Gradient Descent\n",
    "for i in range(iterations):\n",
    "    j = random.randint(0,len(X)-1)\n",
    "    _x, _y = X[j], Y[j]\n",
    "    \n",
    "    # slope with respect to m\n",
    "    D_m = 2 * _x * ((m * _x + b) - _y)\n",
    "\n",
    "    # slope with respect to b\n",
    "    D_b = 2 * ((m * _x + b) - _y)\n",
    "\n",
    "    # update m and b\n",
    "    m -= L * D_m\n",
    "    b -= L * D_b\n",
    "    \n",
    "print(m, b)\n",
    "\n",
    "# plot the result \n",
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.plot(X, m*X+b)\n",
    "plt.plot(X, Y, 'o') # scatterplot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db51bb4-5c79-4535-9e34-3707d44c82c7",
   "metadata": {},
   "source": [
    "## EXERCISE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b84341-069c-43d6-b092-77863a8616df",
   "metadata": {},
   "source": [
    "Below we have a funciton that accepts input variables $ x $ and $ y $. \n",
    "\n",
    "$ \\Large f(x,y) = 3 \\left(x + 2\\right)^{2} + 0.5 \\left(y - 1\\right)^{2} $\n",
    "\n",
    "Find the $ x $ and $ y $ values that produces the lowest value in that function using gradient descent, and then plot it. Fill in the code below by replacing the question marks \"?\" and experimenting with the learning rate and iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dff00-c695-46ee-a404-b397c4280a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "import random \n",
    "\n",
    "# declare function and derivative\n",
    "x,y = symbols('x y')\n",
    "f = 3*(x+2)**2 + .5*(y-1)**2\n",
    "dx = diff(f, x) \n",
    "dy = diff(f, y)\n",
    "\n",
    "# declare learning rate \n",
    "L = ?\n",
    "\n",
    "# start x at random location \n",
    "x_i = random.uniform(-10,10)\n",
    "y_i = random.uniform(-10,10)\n",
    "\n",
    "for i in range(?):\n",
    "    dx_i = dx.subs(x, x_i)\n",
    "    dy_i = dy.subs(y, y_i) \n",
    "\n",
    "    x_i -= dx_i * L \n",
    "    y_i -= dy_i * L \n",
    "\n",
    "    b = -(dx_i * x_i + dy_i * y_i - f.subs([(x, x_i), (y, y_i)]))\n",
    "\n",
    "# print and plot result \n",
    "print(f\"x = {x_i}, y = {y_i}\")\n",
    "plot3d(f, dx_i*x + dy_i * y + b, xlim=(-10,10),ylim=(-10,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a6e0d0-49f6-48e3-8096-36ca92cfad64",
   "metadata": {},
   "source": [
    "### SCROLL DOWN FOR ANSWER\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "|<br>\n",
    "v "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af617887-fc8c-4b92-818b-78bc61d27fee",
   "metadata": {},
   "source": [
    "You should converge at $ x = -2 $ and $ y = 1 $. A learning rate of $ .05 $ and $ 1000 $ iterations should be sufficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c1978-6859-4c53-8262-6f7edc584033",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import * \n",
    "from sympy.plotting import plot3d\n",
    "import random \n",
    "\n",
    "# declare function and derivative\n",
    "x,y = symbols('x y')\n",
    "f = 3*(x+2)**2 + .5*(y-1)**2\n",
    "dx = diff(f, x) \n",
    "dy = diff(f, y)\n",
    "\n",
    "# declare learning rate \n",
    "L = .05\n",
    "\n",
    "# start x at random location \n",
    "x_i = random.uniform(-10,10)\n",
    "y_i = random.uniform(-10,10)\n",
    "\n",
    "for i in range(1000):\n",
    "    dx_i = dx.subs(x, x_i)\n",
    "    dy_i = dy.subs(y, y_i) \n",
    "\n",
    "    x_i -= dx_i * L \n",
    "    y_i -= dy_i * L \n",
    "\n",
    "    b = -(dx_i * x_i + dy_i * y_i - f.subs([(x, x_i), (y, y_i)]))\n",
    "\n",
    "# print and plot result \n",
    "print(f\"x = {x_i}, y = {y_i}\")\n",
    "plot3d(f, dx_i*x + dy_i * y + b, xlim=(-10,10),ylim=(-10,10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
